{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "print('1')\n",
    "data=np.genfromtxt(filename,delimiter=',')\n",
    "m0,n0=np.shape(data)\n",
    "X0=data[:,0:n0-1]\n",
    "m,n=np.shape(X0)   \n",
    "y0=np.array([data[:,n0-1]]).T\n",
    "idx=np.array(range(m))\n",
    "random.shuffle(idx)\n",
    "X=np.zeros((m,n))\n",
    "y=np.zeros((m,1))\n",
    "for i in range(m):\n",
    "    X[i,:]=X0[idx[i],:]\n",
    "    y[i,0]=y0[idx[i],0]\n",
    "\n",
    "\n",
    "\n",
    "# initializing the weights and biases\n",
    "l=3  #total number of layers excluding the output layer\n",
    "layer_unit=np.zeros((l+1,1))\n",
    "layer_unit[0,0]=n\n",
    "print('enter the number of units or neurons in each layer')\n",
    "B=['b' for i in range(l)]\n",
    "for i in range(1,l+1):\n",
    "    a=input()\n",
    "    layer_unit[i,0]=int(a)\n",
    "    B[i-1]=string.ascii_letters[i-1]\n",
    "layer_unit=np.array(layer_unit,dtype=np.int32)\n",
    "W={}\n",
    "for i in range(l):\n",
    "    W[B[i]]=np.random.random((layer_unit[i+1,0],layer_unit[i,0]+1))\n",
    "\n",
    "y_0=np.zeros((m,layer_unit[l,0]))\n",
    "for i in range(layer_unit[l,0]):\n",
    "    r=np.array(y==i,dtype=np.int32)\n",
    "    y_0[:,i]=r[:,0]\n",
    "\n",
    "print(np.shape(y_0))\n",
    "\n",
    "# sigmoid activation function\n",
    "\n",
    "def sigmoid(z):\n",
    "    a=(np.ones((np.shape(z))))/((np.ones((np.shape(z))))+np.exp((-1)*(z)))\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "# implementing front propagation\n",
    "\n",
    "def activation(X,W,m,l,layer_unit):\n",
    "   A={}\n",
    "   for i in range(l):\n",
    "       A[B[i]] = np.zeros((m, layer_unit[i + 1,0]))\n",
    "   A[B[0]]=sigmoid(np.dot(np.hstack([np.ones((m,1)),X]),W[B[0]].T))\n",
    "   for i in range(1,l):\n",
    "       A[B[i]] = sigmoid(np.dot(np.hstack([np.ones((m,1)),A[B[i-1]]]),W[B[i]].T))\n",
    "   return(A)\n",
    "\n",
    "# costfunction\n",
    "\n",
    "\n",
    "def costfunc (a,y,m,lam,layer_unit):\n",
    "    sum0=-(np.sum((np.dot(y,np.log(a).T))))-(np.sum(np.dot((np.ones((np.shape(y)))-y),(np.log(np.ones((np.shape(a)))-a)).T)))\n",
    "    sum1=0\n",
    "    if(lam!=0):\n",
    "      for i in range(l):\n",
    "          w=W[B[i]]\n",
    "          sum1=sum1+np.sum(w[:,1:layer_unit[i,0]+1]**2)\n",
    "    cost=(1/m)*(sum0)+(lam/(2*m))*(sum1)\n",
    "    return cost\n",
    "\n",
    "# implementing backpropagation\n",
    "\n",
    "def gradient(X,y,W,m,l,lam,layer_unit):\n",
    "    del0={}\n",
    "    Del={}\n",
    "    W_grad={}\n",
    "    for i in range(l):\n",
    "        del0[B[i]] = np.zeros((m, layer_unit[i + 1,0]))\n",
    "        Del[B[i]] = np.zeros((layer_unit[i + 1,0], (layer_unit[i,0] + 1)))\n",
    "        W_grad[B[i]] = np.zeros((layer_unit[i + 1,0], (layer_unit[i,0] + 1)))\n",
    "\n",
    "    A=activation(X,W,m,l,layer_unit)\n",
    "    for i in range(l):\n",
    "        if(i==0):\n",
    "            del0[B[l-1]]=A[B[l-1]]-y\n",
    "        else:\n",
    "            a=np.hstack([np.ones((m,1)),A[B[l-1-i]]])\n",
    "            r=np.dot(del0[B[l-i]], W[B[l-i]]) * (a) * (np.ones((np.shape(a))) - a)\n",
    "            u,v=np.shape(r)\n",
    "            del0[B[l - 1 - i]]=r[:,1:v]\n",
    "    for i in range(l):\n",
    "        if(i==0):\n",
    "            Del[B[i]]=np.dot(del0[B[i]].T,np.hstack([np.ones((m,1)),X]))\n",
    "        else:\n",
    "            Del[B[i]] = np.dot(del0[B[i]].T,np.hstack([np.ones((m,1)),A[B[i-1]]]))\n",
    "\n",
    "    for i in range(l):\n",
    "        w=W[B[i]]\n",
    "        d=Del[B[i]]\n",
    "        grad1 = (1 / m) *((d[:,1:(layer_unit[i,0]+1)]) + (lam) *(w[:,1:(layer_unit[i,0]+1)]))\n",
    "        grad0=(1/m)*(np.array([d[:,0]]).T)\n",
    "        W_grad[B[i]]=np.hstack([grad0,grad1])\n",
    "    return W_grad\n",
    "\n",
    "#optimizing the parameters\n",
    "\n",
    "def gradient_descent(X,y,m,W,l,max_iter,alpha,lam,layer_unit):\n",
    "    p=int(m/10)\n",
    "    q=m%10\n",
    "    if(q==0):\n",
    "       costf=np.zeros((p*max_iter,1))\n",
    "    else:\n",
    "       costf = np.zeros(((p+1) * max_iter, 1))\n",
    "    o=0\n",
    "    for i in range(max_iter):\n",
    "        for j in range(p):\n",
    "            x=X[10*j:10*(j+1),:]\n",
    "            Y=y[10*j:10*(j+1),:]\n",
    "            W_grad=gradient(x,Y,W,10,l,lam,layer_unit)\n",
    "            for u in range(l):\n",
    "                W[B[u]]=W[B[u]]-((alpha)*(W_grad[B[u]]))\n",
    "            A = activation(x, W, 10, l,layer_unit)\n",
    "            a = A[B[l - 1]]\n",
    "            costf[o,0]=costfunc(a,Y,10,lam,layer_unit)\n",
    "            o += 1\n",
    "            print('iteration',o)\n",
    "        if(q!=0):\n",
    "          x=X[10*p:10*p+q,:]\n",
    "          Y = y[10*p:10*p+q,:]\n",
    "          W_grad = gradient(x, Y, W, q, l,lam,layer_unit)\n",
    "          for u in range(l):\n",
    "              W[B[u]] = W[B[u]] - ((alpha) * (W_grad[B[u]]))\n",
    "          A = activation(x, W, q, l,layer_unit)\n",
    "          a = A[B[l - 1]]\n",
    "          costf[o,0] = costfunc(a, Y, q,lam,layer_unit)\n",
    "          o+=1\n",
    "          print('iteration', o)\n",
    "    if(q==0):\n",
    "      p = np.array(range(1, p*max_iter + 1))\n",
    "    else:\n",
    "      p = np.array(range(1, (p+1) * max_iter +1))\n",
    "    plt.plot(p, costf)\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Value of Cost Function')\n",
    "    plt.title('The variation of the cost function with the number of iterations')\n",
    "    plt.show()\n",
    "    return(W)\n",
    "\n",
    "\n",
    "def accuracy(A,y,m):\n",
    "    e=np.argmax(A[B[l-1]],1)\n",
    "    e=np.array([e]).T\n",
    "    r=np.array(e==y,dtype=np.int32)\n",
    "    accuracy=(np.sum(r))/m\n",
    "    return(accuracy)\n",
    "\n",
    "\n",
    "def dec_boundary(x_start,y_start,x_end,y_end,xi,yi,W,layer_unit,l):\n",
    "    x1=x_start\n",
    "    y1=y_start\n",
    "    X=[[x1]]\n",
    "    y=[[y1]]\n",
    "    while(x1<x_end):\n",
    "        x1+=xi\n",
    "        X.append([x1])\n",
    "        y1+=yi\n",
    "        y.append([y1])\n",
    "    X=np.array(X)\n",
    "    y=np.array(y)\n",
    "    m,n=np.shape(X)\n",
    "    A=activation(X,W,m,l,layer_unit)\n",
    "    u=['*','+','#','-']\n",
    "    q=['r','k','y','b']\n",
    "    e = np.argmax(A[B[l - 1]], 1)\n",
    "    e=np.array([e]).T\n",
    "    for i in range(layer_unit[l,0]):\n",
    "       r=np.array(e==i,np.int32)\n",
    "       a,b=np.array(np.nonzero(r))\n",
    "       x=np.zeros((np.shape(a)[0],n))\n",
    "       for j in range(np.shape(a)[0]):\n",
    "           x[j,:]=np.array([X[a[j],:]])\n",
    "       plt.scatter(x[:,0],x[:,1],marker=u[i],color=q[i],s=30)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
